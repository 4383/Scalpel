# (generated with --quick)

import __future__
from typing import Any, Dict, Tuple, Type, TypeVar

absolute_import: __future__._Feature
collections: module
copy: module
division: __future__._Feature
json: module
math: module
np: module
print_function: __future__._Feature
re: module
six: module
tf: Any

_TBertConfig = TypeVar('_TBertConfig', bound=BertConfig)

class BertConfig:
    __doc__: str
    attention_probs_dropout_prob: Any
    hidden_act: Any
    hidden_dropout_prob: Any
    hidden_size: Any
    initializer_range: Any
    intermediate_size: Any
    max_position_embeddings: Any
    num_attention_heads: Any
    num_hidden_layers: Any
    type_vocab_size: Any
    vocab_size: Any
    def __init__(self, vocab_size, hidden_size = ..., num_hidden_layers = ..., num_attention_heads = ..., intermediate_size = ..., hidden_act = ..., hidden_dropout_prob = ..., attention_probs_dropout_prob = ..., max_position_embeddings = ..., type_vocab_size = ..., initializer_range = ...) -> None: ...
    @classmethod
    def from_dict(cls: Type[_TBertConfig], json_object) -> _TBertConfig: ...
    @classmethod
    def from_json_file(cls: Type[_TBertConfig], json_file) -> _TBertConfig: ...
    def to_dict(self) -> dict: ...
    def to_json_string(self) -> str: ...

class BertModel:
    __doc__: str
    all_encoder_layers: list
    embedding_output: Any
    embedding_table: Any
    pooled_output: Any
    sequence_output: Any
    def __init__(self, config, is_training, input_ids, input_mask = ..., token_type_ids = ..., use_one_hot_embeddings = ..., scope = ...) -> None: ...
    def get_all_encoder_layers(self) -> list: ...
    def get_embedding_output(self) -> Any: ...
    def get_embedding_table(self) -> Any: ...
    def get_pooled_output(self) -> Any: ...
    def get_sequence_output(self) -> Any: ...

def assert_rank(tensor, expected_rank, name = ...) -> None: ...
def attention_layer(from_tensor, to_tensor, attention_mask = ..., num_attention_heads = ..., size_per_head = ..., query_act = ..., key_act = ..., value_act = ..., attention_probs_dropout_prob = ..., initializer_range = ..., do_return_2d_tensor = ..., batch_size = ..., from_seq_length = ..., to_seq_length = ...) -> Any: ...
def create_attention_mask_from_input_mask(from_tensor, to_mask) -> Any: ...
def create_initializer(initializer_range = ...) -> Any: ...
def dropout(input_tensor, dropout_prob) -> Any: ...
def embedding_lookup(input_ids, vocab_size, embedding_size = ..., initializer_range = ..., word_embedding_name = ..., use_one_hot_embeddings = ...) -> Tuple[Any, Any]: ...
def embedding_postprocessor(input_tensor, use_token_type = ..., token_type_ids = ..., token_type_vocab_size = ..., token_type_embedding_name = ..., use_position_embeddings = ..., position_embedding_name = ..., initializer_range = ..., max_position_embeddings = ..., dropout_prob = ...) -> Any: ...
def gelu(x) -> Any: ...
def get_activation(activation_string) -> Any: ...
def get_assignment_map_from_checkpoint(tvars, init_checkpoint) -> Tuple[collections.OrderedDict, Dict[Any, int]]: ...
def get_shape_list(tensor, expected_rank = ..., name = ...) -> Any: ...
def layer_norm(input_tensor, name = ...) -> Any: ...
def layer_norm_and_dropout(input_tensor, dropout_prob, name = ...) -> Any: ...
def reshape_from_matrix(output_tensor, orig_shape_list) -> Any: ...
def reshape_to_matrix(input_tensor) -> Any: ...
def transformer_model(input_tensor, attention_mask = ..., hidden_size = ..., num_hidden_layers = ..., num_attention_heads = ..., intermediate_size = ..., intermediate_act_fn = ..., hidden_dropout_prob = ..., attention_probs_dropout_prob = ..., initializer_range = ..., do_return_all_layers = ...) -> Any: ...
