# (generated with --quick)

import __future__
from typing import Any, Dict, List

absolute_import: __future__._Feature
collections: module
division: __future__._Feature
print_function: __future__._Feature
re: module
six: module
tf: Any
unicodedata: module

class BasicTokenizer:
    __doc__: str
    do_lower_case: Any
    def __init__(self, do_lower_case = ...) -> None: ...
    def _clean_text(self, text) -> str: ...
    def _is_chinese_char(self, cp) -> bool: ...
    def _run_split_on_punc(self, text) -> List[str]: ...
    def _run_strip_accents(self, text) -> str: ...
    def _tokenize_chinese_chars(self, text) -> str: ...
    def tokenize(self, text) -> List[str]: ...

class FullTokenizer:
    __doc__: str
    basic_tokenizer: BasicTokenizer
    inv_vocab: Dict[int, Any]
    vocab: collections.OrderedDict[Any, int]
    wordpiece_tokenizer: WordpieceTokenizer
    def __init__(self, vocab_file, do_lower_case = ...) -> None: ...
    def convert_ids_to_tokens(self, ids) -> list: ...
    def convert_tokens_to_ids(self, tokens) -> List[int]: ...
    def tokenize(self, text) -> List[str]: ...

class WordpieceTokenizer:
    __doc__: str
    max_input_chars_per_word: Any
    unk_token: Any
    vocab: Any
    def __init__(self, vocab, unk_token = ..., max_input_chars_per_word = ...) -> None: ...
    def tokenize(self, text) -> list: ...

def _is_control(char) -> bool: ...
def _is_punctuation(char) -> bool: ...
def _is_whitespace(char) -> bool: ...
def convert_by_vocab(vocab, items) -> list: ...
def convert_ids_to_tokens(inv_vocab, ids) -> list: ...
def convert_to_unicode(text) -> Any: ...
def convert_tokens_to_ids(vocab, tokens) -> list: ...
def load_vocab(vocab_file) -> collections.OrderedDict[Any, int]: ...
def printable_text(text) -> Any: ...
def validate_case_matches_checkpoint(do_lower_case, init_checkpoint) -> None: ...
def whitespace_tokenize(text) -> Any: ...
